{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Character Classifier Using A Deep Convolutional Neural Network\n",
    "![LeNet Architecture](lenet.png)\n",
    "Source: Yan LeCun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load MNIST Data\n",
    "\n",
    "Load the MNIST data, which comes pre-loaded with TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "The training, validation, and testing feature sets lengths match the their corresponding label sets\n",
      "\n",
      "Image Shape: (28, 28, 1)\n",
      "\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST Dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Save MNIST Dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False)\n",
    "\n",
    "# Save training, validation, and test sets\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "\n",
    "# Verify that number of images in each set matches the number of labels in the same set\n",
    "assert(len(X_train) == len(y_train))\n",
    "assert(len(X_validation) == len(y_validation))\n",
    "assert(len(X_test) == len(y_test))\n",
    "\n",
    "# Print out the assertion results\n",
    "print()\n",
    "print(\"The training, validation, and testing feature sets lengths match the their corresponding label sets\")\n",
    "\n",
    "# Print out the shape of one image so that we know what the dimensions of the data are\n",
    "print()\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print()\n",
    "\n",
    "# Print out the size of each set\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The MNIST data that TensorFlow pre-loads comes as 28x28x1 images.\n",
    "\n",
    "However, the LeNet architecture only accepts 32x32xC images, where C is the number of color channels.\n",
    "\n",
    "In order to reformat the MNIST data into a shape that LeNet will accept, we pad the data with two rows of zeros on the top and bottom, and two columns of zeros on the left and right (28+2+2 = 32).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Image Shape: (32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pad images with 0s\n",
    "# Transform 28x28 images into 32x32 images (by adding padding) that LeNet can process\n",
    "X_train      = np.pad(X_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "X_validation = np.pad(X_validation, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "X_test       = np.pad(X_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "\n",
    "print(\"Updated Image Shape: {}\".format(X_train[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Visualize Data\n",
    "\n",
    "View a sample from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices: 3273, 4058, 44654, 3273, 4058, 44654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faf44b2feb8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD7CAYAAAB68m/qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faf67285048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Select random images from training set so we can\n",
    "# see the size of our data, and print out the labels\n",
    "index1 = random.randint(0, len(X_train))\n",
    "image1 = X_train[index1].squeeze()\n",
    "index2 = random.randint(0, len(X_train))\n",
    "image2 = X_train[index2].squeeze()\n",
    "index3 = random.randint(0, len(X_train))\n",
    "image3 = X_train[index3].squeeze()\n",
    "index4 = random.randint(0, len(X_train))\n",
    "image4 = X_train[index4].squeeze()\n",
    "index5 = random.randint(0, len(X_train))\n",
    "image5 = X_train[index5].squeeze()\n",
    "index6 = random.randint(0, len(X_train))\n",
    "image6 = X_train[index6].squeeze()\n",
    "\n",
    "# Plot randomly selected images from training set\n",
    "fig = plt.figure()\n",
    "print(\"Indices: \"+str(index1) +\", \"+str(index2)+\", \"+str(index3)+\", \"+str(index1) +\", \"+str(index2)+\", \"+str(index3))\n",
    "fig.add_subplot(2,3,1)\n",
    "plt.imshow(image1.squeeze(), cmap=\"gray\")\n",
    "fig.add_subplot(2,3,2)\n",
    "plt.imshow(image2.squeeze(), cmap=\"gray\")\n",
    "fig.add_subplot(2,3,3)\n",
    "plt.imshow(image3.squeeze(), cmap=\"gray\")\n",
    "fig.add_subplot(2,3,4)\n",
    "plt.imshow(image4.squeeze(), cmap=\"gray\")\n",
    "fig.add_subplot(2,3,5)\n",
    "plt.imshow(image5.squeeze(), cmap=\"gray\")\n",
    "fig.add_subplot(2,3,6)\n",
    "plt.imshow(image6.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Shuffle the training data so that the model does not rely on the order of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been shuffled\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Shuffle the training set so that we never train in the same order\n",
    "# This is important because we don't want to have to rely on the ordering of data\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "print(\"Data has been shuffled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Setup TensorFlow\n",
    "The `EPOCH` and `BATCH_SIZE` values affect the training speed and model accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will train on 10 with a batch size of 128\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Start building DNN\n",
    "\n",
    "# EPOCHS tells TF how many times to run our training data through the network\n",
    "# The more EPOCHS, the better our model will train but also the longer it will take\n",
    "EPOCHS = 10\n",
    "\n",
    "# Tells TF how many taining images to run through the network at a time\n",
    "# The larger the batch_size, the faster the model will train,\n",
    "# but our processer may have a limit on how large of a batch it can run\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "print(\"We will train on\",EPOCHS,\"with a batch size of\",BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement LeNet-5\n",
    "Implement the [LeNet-5](http://yann.lecun.com/exdb/lenet/) neural network architecture.\n",
    "\n",
    "### Input\n",
    "The LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "### Architecture\n",
    "**Layer 1: Convolutional.** The output shape should be 28x28x6.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Pooling.** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2: Convolutional.** The output shape should be 10x10x16.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Pooling.** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten.** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D. The easiest way to do is by using `tf.contrib.layers.flatten`, which is already imported for you.\n",
    "\n",
    "**Layer 3: Fully Connected.** This should have 120 outputs.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Layer 4: Fully Connected.** This should have 84 outputs.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Layer 5: Fully Connected (Logits).** This should have 10 outputs.\n",
    "\n",
    "### Output\n",
    "Return the result of the 2nd fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Character Classifier model has been defined\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def Character_Classifier(x):    \n",
    "    # Arguments used for tf.truncated_normal,\n",
    "    # randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    # This layer has a 5x5 filter with an input depth of 1 and an output depth of 6\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma)) #Weights\n",
    "    conv1_b = tf.Variable(tf.zeros(6)) #Biases\n",
    "    # Now we use the conv2d function to convolve the filter over the images, and we add the baises at the end\n",
    "    # The formula for convolutions tells us that the output height = the input height minus the filter height + 1\n",
    "    # all divided by the vertical stride. In this case: (32 - 5 + 1)/1 = 28 (This also works for output width)\n",
    "    # Convolutional layer output = 28x28x6\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # Activation.\n",
    "    # Activate the output of the convollutional layer (28x28x6) with a ReLU activation function\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    # Pool the output using a 2x2 kernel with a 2x2 stride which gives us a pooling output of 14x14x6\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    \n",
    "    # The network then runs through another set of convolutional activation and pooling layers giving and output of 5x5x16\n",
    "    \n",
    "    # Layer 2: Convolutional. Output = 10x10x16.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # Flatten. Input = 5x5x16. Output = 400.\n",
    "    # We then flatten this output (5x5x16) into a vector of length 5x5x16 which equals 400\n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    # Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    # We pass our last output into this fully connected layer with a width of 120\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # Activation.\n",
    "    # We apply a ReLU activation to the ouput of the fully connected layer\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "\n",
    "    # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    # We repeat this pattern, but this time for a layer with width 84\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(84))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "\n",
    "    # Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "    # Finally, we attach a fully connected output layer with a width equal to\n",
    "    # the number of classes in our label set, in this case: 10, one for each digit\n",
    "    # The width of the output layer is 10. This output is also known as our logits that we return\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 10), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(10))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits\n",
    "\n",
    "print(\"The Character Classifier model has been defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Features and Labels\n",
    "Train LeNet to classify [MNIST](http://yann.lecun.com/exdb/mnist/) data.\n",
    "\n",
    "`x` is a placeholder for a batch of input images.\n",
    "`y` is a placeholder for a batch of output labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variable x will hold our input batches\n",
      "The variable y will hold our labels\n",
      "The variable one_hot_y will encode each label with a numeric id\n"
     ]
    }
   ],
   "source": [
    "# Here we set up our tensorFlow variables\n",
    "\n",
    "# x is a placeholder that will store our input batches\n",
    "# We initialize the batch size to none which allows the placeholder to\n",
    "# later accept a batch of any size, and we set the image dimensions to 32x32x1\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "\n",
    "# y stores our labels in this case our labels come through with sparse variables\n",
    "# which just means that they are integers, they aren't one-hot encoded yet\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "\n",
    "# We use the tf.ont_hot() function to one-hot encode the labels\n",
    "one_hot_y = tf.one_hot(y, 10)\n",
    "\n",
    "print(\"The variable x will hold our input batches\")\n",
    "print(\"The variable y will hold our labels\")\n",
    "print(\"The variable one_hot_y will encode each label with a numeric id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training Pipeline\n",
    "Create a training pipeline that uses the model to classify MNIST data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pipeline defined\n"
     ]
    }
   ],
   "source": [
    "# Now we can set up our training pipeline\n",
    "\n",
    "# The learning rate tells tf how quickly to update the networks weights\n",
    "rate = 0.001\n",
    "\n",
    "# We pass the input data to the LeNet function to calculate our logits\n",
    "logits = Character_Classifier(x)\n",
    "\n",
    "# We use the below function to compare our logits to the ground truth labels\n",
    "# and calculate the cross entropy (a measure of how different the different\n",
    "# the logits are from the ground truth training labels)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "\n",
    "# The tf reduced mean function averages the cross entropy from all the training images\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Adam optimizer uses an algorithm to minimize the loss function\n",
    "# Similar to stochastic gradient descent, but a little more complex\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "\n",
    "# We run the minimize function on the optimizer which uses back propagation\n",
    "# to update the network and minimize our training loss\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "print(\"Training pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model Evaluation\n",
    "Evaluate how well the loss and accuracy of the model for a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation method defined\n"
     ]
    }
   ],
   "source": [
    "# In this code cell we set up another pipeline, this time for evaluating the model\n",
    "# The training pipeline above trains the model, but the evaluation pipeline\n",
    "# we create here will evaluate how good the model is\n",
    "\n",
    "# The first step is to measure whether a given prediction is correct by\n",
    "# comparing the logit prediction to the one-hot encoded ground truth label\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "\n",
    "# The second step in the pipeline is to compute the models overall accuracy\n",
    "# by averaging the individual prediction accuracies \n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# We use the evaluate function to run evaluation pipeline\n",
    "# The function takes a dataset as input. It averages the accuracy of each\n",
    "# batch to calculate the total accuracy of the model\n",
    "def evaluate(X_data, y_data):\n",
    "    # It sets some initial variables\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    # Batches the data set and runs it through the evaluation pipeline\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "\n",
    "print(\"Evaluation method defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Model\n",
    "Run the training data through the training pipeline to train the model.\n",
    "\n",
    "Before each epoch, shuffle the training set.\n",
    "\n",
    "After each epoch, measure the loss and accuracy of the validation set.\n",
    "\n",
    "Save the model after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.107\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.113\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.098\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Accuracy = 0.096\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Accuracy = 0.113\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation Accuracy = 0.110\n",
      "\n",
      "EPOCH 7 ...\n",
      "Validation Accuracy = 0.099\n",
      "\n",
      "EPOCH 8 ...\n",
      "Validation Accuracy = 0.113\n",
      "\n",
      "EPOCH 9 ...\n",
      "Validation Accuracy = 0.113\n",
      "\n",
      "EPOCH 10 ...\n",
      "Validation Accuracy = 0.113\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Now that everything else is set up we can build a function to train and\n",
    "# validate our model\n",
    "\n",
    "# First we create the TF session and initialize the variables\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    # We train over whatever number of EPOCHs has been set\n",
    "    for i in range(EPOCHS):\n",
    "        # At the beginning of each EPOCH we shuffle our training data to\n",
    "        # ensure that our training isn't bias by the order of the images\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        # Then we break our training data into batches and train the model on each batch\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        \n",
    "        # At the end of each EPOCH we evaluate the model on our validation data\n",
    "        validation_accuracy = evaluate(X_validation, y_validation)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "    \n",
    "    # Once we have completely trained the model we save it so that\n",
    "    # we can load it up later and modify it, or evaluate it on our test dataset\n",
    "    saver.save(sess, './lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluate the Model\n",
    "Once you are completely satisfied with your model, evaluate the performance of the model on the test set.\n",
    "\n",
    "Be sure to only do this once!\n",
    "\n",
    "If you were to measure the performance of your trained model on the test set, then improve your model, and then measure the performance of your model on the test set again, that would invalidate your test results. You wouldn't get a true measure of how well your model would perform against real data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.114\n",
      "Validation Accuracy = 0.113\n"
     ]
    }
   ],
   "source": [
    "# This last code cell evaluates the model on our test data set\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    validation_accuracy = evaluate(X_validation, y_validation)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n",
    "    print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
